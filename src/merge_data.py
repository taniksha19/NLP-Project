import pandas as pd
import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from tqdm import tqdm
import gc

# ==========================================
# CONFIGURATION
# ==========================================
MODEL_NAME = "s-nlp/mt0-xl-detox-orpo"
BATCH_SIZE = 8 
OUTPUT_FILE = "final_detox_dataset.csv"

# Language codes available in the paradetox dataset
PARADETOX_LANGS = ['en', 'ru', 'uk', 'de', 'es', 'am', 'zh', 'ar', 'hi']

# ==========================================
# PART 1: LOAD & MERGE DATASETS
# ==========================================
def prepare_datasets():
    print("\n--- PART 1: Loading Datasets ---")
    
    # -------------------------------------------------------
    # 1. Load Paradetox (FIX: Load each language split)
    # -------------------------------------------------------
    print("Loading textdetox/multilingual_paradetox...")
    all_para_dfs = []
    
    for lang in PARADETOX_LANGS:
        try:
            print(f"   - Loading language: {lang}")
            ds_lang = load_dataset("textdetox/multilingual_paradetox", split=lang)
            df_lang = ds_lang.to_pandas()
            
            # Add language column
            df_lang['lang'] = lang
            
            # Normalize column names
            rename_map = {}
            if "toxic_sentence" in df_lang.columns:
                rename_map["toxic_sentence"] = "toxic_sentence"
            if "neutral_sentence" in df_lang.columns:
                rename_map["neutral_sentence"] = "non_toxic_sentence"
            elif "non_toxic_sentence" not in df_lang.columns and "neutral_sentence" not in df_lang.columns:
                # Check for alternative names
                for col in df_lang.columns:
                    if 'neutral' in col.lower() or 'detox' in col.lower():
                        rename_map[col] = "non_toxic_sentence"
                        break
            
            if rename_map:
                df_lang = df_lang.rename(columns=rename_map)
            
            # Ensure we have the required columns
            if 'toxic_sentence' in df_lang.columns and 'non_toxic_sentence' in df_lang.columns:
                df_lang = df_lang[['toxic_sentence', 'non_toxic_sentence', 'lang']]
                all_para_dfs.append(df_lang)
                print(f"      ✓ Loaded {len(df_lang)} rows")
            else:
                print(f"      ⚠ Skipping {lang}: missing required columns")
                
        except Exception as e:
            print(f"      ⚠ Could not load {lang}: {e}")
    
    if not all_para_dfs:
        print("CRITICAL ERROR: No paradetox data loaded!")
        return None
    
    # Combine all language dataframes
    df_para = pd.concat(all_para_dfs, ignore_index=True)
    print(f"\n✓ Total Paradetox rows loaded: {len(df_para)}")

    # -------------------------------------------------------
    # 2. Load Toxicity Dataset (Input Only) - Language by Language
    # -------------------------------------------------------
    print("\nLoading additional toxicity dataset (textdetox/multilingual_toxicity_dataset)...")
    all_tox_dfs = []
    
    # Available languages in the toxicity dataset (more than paradetox)
    TOXICITY_LANGS = ['en', 'ru', 'uk', 'de', 'es', 'am', 'zh', 'ar', 'hi', 'it', 'fr', 'he', 'hin', 'tt', 'ja']
    
    for lang in TOXICITY_LANGS:
        try:
            print(f"   - Loading toxicity data for: {lang}")
            ds_tox_lang = load_dataset("textdetox/multilingual_toxicity_dataset", split=lang)
            df_tox_lang = ds_tox_lang.to_pandas()
            
            # Add language column
            df_tox_lang['lang'] = lang
            
            # Filter for toxic rows (is_toxic == 1)
            if 'is_toxic' in df_tox_lang.columns:
                df_tox_lang = df_tox_lang[df_tox_lang['is_toxic'] == 1].copy()
            elif 'toxic' in df_tox_lang.columns:
                df_tox_lang = df_tox_lang[df_tox_lang['toxic'] == 1].copy()
            
            # Normalize Text Column
            text_col_found = False
            for col in ['text', 'comment_text', 'sentence', 'content', 'toxic_comment']:
                if col in df_tox_lang.columns:
                    df_tox_lang = df_tox_lang.rename(columns={col: "toxic_sentence"})
                    text_col_found = True
                    break
            
            if not text_col_found:
                print(f"      ⚠ No text column found in {lang}, skipping")
                continue
            
            # Prepare Target Column (Empty - will be generated by model)
            df_tox_lang['non_toxic_sentence'] = None
            
            # Keep only needed columns
            df_tox_lang = df_tox_lang[['toxic_sentence', 'non_toxic_sentence', 'lang']]
            all_tox_dfs.append(df_tox_lang)
            print(f"      ✓ Loaded {len(df_tox_lang)} toxic rows")
            
        except Exception as e:
            print(f"      ⚠ Could not load {lang}: {e}")
    
    # Combine all toxicity data
    if all_tox_dfs:
        df_tox_filtered = pd.concat(all_tox_dfs, ignore_index=True)
        print(f"\n✓ Total toxicity rows loaded: {len(df_tox_filtered)}")
    else:
        df_tox_filtered = pd.DataFrame()
        print(f"\n⚠ No toxicity data loaded")

    # -------------------------------------------------------
    # 3. Merge
    # -------------------------------------------------------
    print("\nMerging datasets...")
    if not df_tox_filtered.empty:
        merged_df = pd.concat([df_para, df_tox_filtered], ignore_index=True)
    else:
        merged_df = df_para
    
    # Deduplicate based on the input sentence
    print(f"Rows before deduplication: {len(merged_df)}")
    merged_df = merged_df.drop_duplicates(subset=['toxic_sentence'])
    
    print(f"✓ Total merged rows after dedup: {len(merged_df)}")
    print(f"✓ Languages present: {merged_df['lang'].unique()}")
    return merged_df

# ==========================================
# PART 2: GENERATE MISSING LABELS (GPU)
# ==========================================
def generate_labels(df):
    print("\n--- PART 2: Generating Missing Labels ---")
    
    # Identify rows that are missing the non-toxic version
    mask = df['non_toxic_sentence'].isna() | (df['non_toxic_sentence'] == "")
    
    if mask.sum() == 0:
        print("✓ No missing labels found. Dataset is complete!")
        return df

    print(f"Rows needing generation: {mask.sum()}")
    print("Loading Model (8-bit quantization for memory efficiency)...")

    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        # 8-bit loading is essential for Colab T4 GPUs
        model = AutoModelForSeq2SeqLM.from_pretrained(
            MODEL_NAME,
            device_map="auto",
            load_in_8bit=True
        )
        print("✓ Model loaded successfully")
    except Exception as e:
        print(f"❌ Model load failed. Error: {e}")
        print("Hint: Ensure 'bitsandbytes' and 'accelerate' are installed.")
        print("Try: !pip install bitsandbytes accelerate")
        return df

    # Prompt Map (language-specific prefixes)
    lang_prompts = {
        'en': 'Detoxify: ', 'ru': 'Детоксифицируй: ', 'uk': 'Детоксифікуй: ',
        'de': 'Detoxifizieren: ', 'es': 'Desintoxicar: ', 'fr': 'Détoxifier: ',
        'ar': 'إزالة السموم: ', 'hi': 'विषाक्तता हटाएँ: ', 'zh': '排毒： ',
        'am': 'Detoxify: ',  # Amharic - using English prefix
        'default': 'Detoxify: '
    }

    # Prepare Batches
    df_process = df[mask].copy()
    toxic_texts = df_process['toxic_sentence'].tolist()
    langs = df_process['lang'].fillna('default').tolist()
    
    generated_results = []
    print("Starting Generation Loop...")
    
    for i in tqdm(range(0, len(toxic_texts), BATCH_SIZE)):
        batch_texts = toxic_texts[i : i + BATCH_SIZE]
        batch_langs = langs[i : i + BATCH_SIZE]
        
        # Construct Inputs with Language Prompts
        input_strings = []
        for t, l in zip(batch_texts, batch_langs):
            prompt = lang_prompts.get(l, lang_prompts['default'])
            input_strings.append(prompt + str(t))
        
        # Tokenize & Generate
        try:
            inputs = tokenizer(input_strings, return_tensors="pt", padding=True, truncation=True, max_length=128).to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=128, num_beams=2)
            
            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            generated_results.extend(decoded)
        except Exception as e:
            print(f"\n⚠ Batch failed: {e}. Filling with empty strings.")
            generated_results.extend([""] * len(batch_texts))
        
        # Memory Management
        if i % 50 == 0:
            torch.cuda.empty_cache()

    # Update DataFrame
    df.loc[mask, 'non_toxic_sentence'] = generated_results
    print(f"\n✓ Generated {len(generated_results)} detoxified versions")
    return df

# ==========================================
# EXECUTION
# ==========================================
if __name__ == "__main__":
    full_df = prepare_datasets()
    
    if full_df is not None:
        final_df = generate_labels(full_df)
        
        final_df.to_csv(OUTPUT_FILE, index=False)
        print(f"\n✅ SUCCESS! Dataset saved to {OUTPUT_FILE}")
        print(f"\nDataset Statistics:")
        print(f"  Total rows: {len(final_df)}")
        print(f"  Languages: {final_df['lang'].value_counts().to_dict()}")
        print(f"\nSample rows:")
        print(final_df.head(3))
    else:
        print("❌ Failed to initialize dataframe.")